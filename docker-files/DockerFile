# VERSION 1.0 (apache-docker)
# AUTHOR: Abhishek Sharma<abhioncbr@yahoo.com>
# DESCRIPTION: docker apache airflow container

FROM python:3.6
MAINTAINER Abhishek Sharma <abhioncbr@yahoo.com>

ARG AIRFLOW_VERSION
ARG AIRFLOW_DEPS="all"
ARG BUILD_DEPS="freetds-dev libkrb5-dev libsasl2-dev libssl-dev libffi-dev libpq-dev git"
ARG OTHER_DEPS="sshpass openssh-server openssh-client less gcc make wget vim curl rsync netcat "
ARG APT_DEPS="$BUILD_DEPS $OTHER_DEPS libsasl2-dev freetds-bin build-essential default-libmysqlclient-dev apt-utils locales "

ENV AIRFLOW_HOME /usr/local/airflow
ENV REDIS_HOME /usr/local/redis
ENV REDIS_DOWNLOAD_URL http://download.redis.io/releases/redis-3.2.5.tar.gz
ENV AIRFLOW_GPL_UNIDECODE yes

#install dependencies packages.
RUN set -x \
    && apt-get update \
    && if [ -n "${APT_DEPS}" ]; then apt-get install -y $APT_DEPS; fi

#Install Redis for celery execution.
#RUN wget -O redis.tar.gz ${REDIS_DOWNLOAD_URL} \
#    && echo "$REDIS_DOWNLOAD_SHA1 *redis.tar.gz" | sha1sum -c - \
#    && mkdir -p ${REDIS_HOME} \
#	&& tar -xzf redis.tar.gz -C ${REDIS_HOME} --strip-components=1 \
#	&& rm redis.tar.gz \
#	&& make -C ${REDIS_HOME} \
#	&& make -C ${REDIS_HOME} install

#Install Redis
RUN apt-get update && apt policy redis-server

#Install java for java based application.
RUN apt-get update && apt policy openjdk-8-jdk

#Instal python dependencies.
RUN if [ -n "${PYTHON_DEPS}" ]; then pip install --no-cache-dir ${PYTHON_DEPS}; fi

#Install Airflow all packages
RUN python -m pip install --upgrade pip setuptools wheel \
    && pip install apache-airflow[$AIRFLOW_DEPS]==$AIRFLOW_VERSION \
    && apt-get clean

RUN groupadd -g 5006 airflow \
    && useradd -ms /bin/bash -d ${AIRFLOW_HOME}  -u 5004 -g 5006 -p airflow airflow

RUN mkdir /code-artifacts /data /home/airflow ${AIRFLOW_HOME}/startup_log ${AIRFLOW_HOME}/.ssh ${AIRFLOW_HOME}/.aws ${AIRFLOW_HOME}/.gcp ${AIRFLOW_HOME}/dags ${AIRFLOW_HOME}/logs ${AIRFLOW_HOME}/plugins \
    && mkdir -p /user/airflow \
#    && mkdir ${AIRFLOW_HOME}/dags/example_dags
#    && cp -r /usr/local/lib/python2.7/dist-packages/airflow/example_dags/* ${AIRFLOW_HOME}/dags/example_dags/ \
    && chown -R airflow:airflow ${AIRFLOW_HOME}/* /data /code-artifacts /user/airflow /home/airflow

ADD script/entrypoint.sh ${AIRFLOW_HOME}/entrypoint.sh
ADD config/airflow.cfg ${AIRFLOW_HOME}/airflow.cfg
ADD config/user_add.py ${AIRFLOW_HOME}/user_add.py
ADD config/ab_user_add.py ${AIRFLOW_HOME}/ab_user_add.py
ADD config/execute_continous_scheduler.sh ${AIRFLOW_HOME}/execute_continous_scheduler.sh

# airflow patch files present in airflowPatch folder
ADD airflowPatch1.10.0/models.py /usr/local/lib/python3.6/site-package/airflow/models.py
RUN chown root:staff /usr/local/lib/python3.6/site-packages/airflow/models.py

ADD airflowPatch1.10.0/password_auth.py /usr/local/lib/python3.6/site-package/airflow/contrib/auth/backends/password_auth.py
RUN chown root:staff /usr/local/lib/python3.6/site-packages/airflow/contrib/auth/backends/password_auth.py

ADD airflowPatch1.10.0/views.py /usr/local/lib/python3.6/site-package/airflow/www/views.py
RUN chown root:staff /usr/local/lib/python3.6/site-packages/airflow/www/views.py

ADD airflowPatch1.10.0/e3a246e0dc1_current_schema.py /usr/local/lib/python3.6/site-package/airflow/migrations/versions/e3a246e0dc1_current_schema.py
RUN chown root:staff /usr/local/lib/python3.6/site-packages/airflow/migrations/versions/e3a246e0dc1_current_schema.py

ADD airflowPatch1.10.0/s3_logger.py /usr/local/lib/python3.6/site-package/airflow/config_templates/s3_logger.py
RUN chown root:staff /usr/local/lib/python3.6/site-package/airflow/config_templates/s3_logger.py


RUN chown -R airflow:airflow ${AIRFLOW_HOME}/*
VOLUME /usr/hdp
VOLUME /code-artifacts
VOLUME ${AIRFLOW_HOME}/.gcp
VOLUME ${AIRFLOW_HOME}/.aws
VOLUME ${AIRFLOW_HOME}/dags
VOLUME ${AIRFLOW_HOME}/logs

USER airflow

ENV PATH=$PATH::/usr/local/gcloud/google-cloud-sdk/bin/
ENV PYTHONPATH=${PYTHONPATH}:/usr/local/lib/python2.7/dist-packages/
#ENV HDP_VERSION=2.6.1.0-129
#export HDP_VERSION=${HDP_VERSION}
#export HADOOP_CONF_DIR=/etc/hadoop/${HDP_VERSION}/0
#export SPARK_CONF_DIR=/etc/spark/${HDP_VERSION}/0
#export HIVE_CONF_DIR=/etc/hive/${HDP_VERSION}/0
#export TEZ_CONF_DIR=/etc/tez/${HDP_VERSION}/0

EXPOSE 5555 8793 2222 6379

WORKDIR ${AIRFLOW_HOME}
ENTRYPOINT ["./entrypoint.sh"]