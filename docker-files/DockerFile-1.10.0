# VERSION 1.0 (airflow version:1.9.0)
# AUTHOR: Abhishek Sharma
# DESCRIPTION: Docker airflow container
# BUILD: docker build --rm -t docker-airflow:latest DockerFile1.9.0

FROM abhioncbr/airflow-base1.10.0
MAINTAINER Abhishek Sharma <abhioncbr@yahoo.com>

USER root

RUN pip install --ignore-installed six \
	&& apt-get update -yqq && apt-get install -yqq mysql-client \
	&& apt-get -o Dpkg::Options::="--force-confmiss" install --reinstall netbase \
	&& pip install google-auth-httplib2

RUN curl https://dl.google.com/dl/cloudsdk/release/google-cloud-sdk.tar.gz > /tmp/google-cloud-sdk.tar.gz
RUN mkdir -p /usr/local/gcloud
RUN tar -C /usr/local/gcloud -xvf /tmp/google-cloud-sdk.tar.gz
RUN /usr/local/gcloud/google-cloud-sdk/install.sh
RUN pip install --upgrade google-api-python-client && pip install google-cloud-storage && pip install boto3
RUN groupadd -g 5006 airflow \
    && useradd -ms /bin/bash -d ${AIRFLOW_HOME}  -u 5004 -g 5006 -p airflow airflow

RUN mkdir /code-artifacts /data /home/airflow ${AIRFLOW_HOME}/startup_log ${AIRFLOW_HOME}/.ssh ${AIRFLOW_HOME}/.aws ${AIRFLOW_HOME}/.gcp ${AIRFLOW_HOME}/dags ${AIRFLOW_HOME}/logs ${AIRFLOW_HOME}/plugins \
    && mkdir -p /user/airflow \
    && mkdir ${AIRFLOW_HOME}/dags/example_dags && cp -r /usr/local/lib/python2.7/dist-packages/airflow/example_dags/* ${AIRFLOW_HOME}/dags/example_dags/ \
    && chown -R airflow:airflow ${AIRFLOW_HOME}/* /data /code-artifacts /user/airflow /home/airflow

ADD script/entrypoint.sh ${AIRFLOW_HOME}/entrypoint.sh
ADD script/entrypoint.sh ${AIRFLOW_HOME}/entrypoint-s3.sh
ADD script/entrypoint.sh ${AIRFLOW_HOME}/entrypoint-gcp.sh
ADD script/entrypoint.sh ${AIRFLOW_HOME}/entrypoint-gcp-s3.sh
ADD config/airflow.cfg ${AIRFLOW_HOME}/airflow.cfg
ADD config/user_add.py ${AIRFLOW_HOME}/user_add.py
ADD config/user_add.py ${AIRFLOW_HOME}/ab_user_add.py
ADD config/execute_continous_scheduler.sh ${AIRFLOW_HOME}/execute_continous_scheduler.sh

# airflow patch files present in airflowPatch folder
ADD airflowPatch1.10.0/models.py /usr/local/lib/python2.7/dist-packages/airflow/models.py
RUN chown root:staff /usr/local/lib/python2.7/dist-packages/airflow/models.py

ADD airflowPatch1.10.0/password_auth.py /usr/local/lib/python2.7/dist-packages/airflow/contrib/auth/backends/password_auth.py
RUN chown root:staff /usr/local/lib/python2.7/dist-packages/airflow/contrib/auth/backends/password_auth.py

ADD airflowPatch1.10.0/views.py /usr/local/lib/python2.7/dist-packages/airflow/www/views.py
RUN chown root:staff /usr/local/lib/python2.7/dist-packages/airflow/www/views.py

ADD airflowPatch1.10.0/e3a246e0dc1_current_schema.py /usr/local/lib/python2.7/dist-packages/airflow/migrations/versions/e3a246e0dc1_current_schema.py
RUN chown root:staff /usr/local/lib/python2.7/dist-packages/airflow/migrations/versions/e3a246e0dc1_current_schema.py

ADD airflowPatch1.10.0/s3_logger.py /usr/local/lib/python2.7/dist-packages/airflow/config_templates/s3_logger.py
RUN chown root:staff /usr/local/lib/python2.7/dist-packages/airflow/config_templates/s3_logger.py

RUN chown -R airflow:airflow ${AIRFLOW_HOME}/*
VOLUME /usr/hdp
VOLUME /user/airflow
VOLUME /code-artifacts
VOLUME ${AIRFLOW_HOME}/logs

USER airflow

ENV PATH=$PATH::/usr/local/gcloud/google-cloud-sdk/bin/
ENV PYTHONPATH=${PYTHONPATH}:/usr/local/lib/python2.7/dist-packages/
#ENV HDP_VERSION=2.6.1.0-129
#export HDP_VERSION=${HDP_VERSION}
#export HADOOP_CONF_DIR=/etc/hadoop/${HDP_VERSION}/0
#export SPARK_CONF_DIR=/etc/spark/${HDP_VERSION}/0
#export HIVE_CONF_DIR=/etc/hive/${HDP_VERSION}/0
#export TEZ_CONF_DIR=/etc/tez/${HDP_VERSION}/0

EXPOSE 5555 8793 2222 6379

WORKDIR ${AIRFLOW_HOME}
ENTRYPOINT ["./entrypoint.sh"]